---
title: "Data Science Capstone Milestone Report"
author: "Tonya MacDonald"
date: "3/18/2021"
output:
  html_document:
    toc: true
    toc_float: true 
    number_sections: true
    theme: cosmo
---

# Introduction

This report is to demonstrate the statistical properties of the text prediction dataset. The report will use exploratory data analysis to describe the features of the training data and then will summarize the plan to create the model in the upcoming project. 

```{r echo=FALSE, message=FALSE, warning=FALSE}

library(knitr)
library(stringi)
library(kableExtra)
library(ggplot2)
library(gridExtra)
#library(RColorBrewer)
library(tm)

rm(list = ls(all.names = TRUE))
setwd("~/git/datasciencecoursera/Capstone")

trainURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
trainDataFile <- "data/Coursera-SwiftKey.zip"

if (!file.exists('data')) {
  dir.create('data')
}

if (!file.exists("data/en_US")) {
  tempFile <- tempfile()
  download.file(trainURL, tempFile)
  unzip(tempFile, exdir = "data")
  unlink(tempFile)
}

# load blogs file
blogsFileName <- "data/en_US/en_US.blogs.txt"
con <- file(blogsFileName, open = "r")
blogs <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)

# load news file
newsFileName <- "data/en_US/en_US.news.txt"
con <- file(newsFileName, open = "r")
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)

# load twitter file
twitterFileName <- "data/en_US/en_US.twitter.txt"
con <- file(twitterFileName, open = "r")
twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
close(con)

rm(con)

# get the file size
fileSizeMB <- round(file.info(c(blogsFileName,
                                newsFileName,
                                twitterFileName))$size / 1024 ^ 2)

# find the number of lines per file
numLines <- sapply(list(blogs, news, twitter), length)

# find the number of  characters per file
numChars <- sapply(list(nchar(blogs), nchar(news), nchar(twitter)), sum)

# find the number of  words per file
numWords <- sapply(list(blogs, news, twitter), stri_stats_latex)[4,]

# find the number of words per line
wpl <- lapply(list(blogs, news, twitter), function(x) stri_count_words(x))

# summarize words per line
wplSummary = sapply(list(blogs, news, twitter),
                    function(x) summary(stri_count_words(x))[c('Min.', 'Mean', 'Max.')])
rownames(wplSummary) = c('Min Words / Line', 'Avg Words / Line', 'Max Words / Line')

summary <- data.frame(
  File = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"),
  FileSize = paste(fileSizeMB, " MB"),
  Lines = numLines,
  Characters = numChars,
  Words = numWords,
  t(rbind(round(wplSummary)))
)

# display in a table
kable(summary,
      row.names = FALSE,
      align = c("l", rep("r", 7)),
      caption = "") %>% kable_styling(position = "left")

```

# Histograms

this is the histogram section

```{r echo=FALSE, message=FALSE, warning=FALSE}

  qplot(wpl[[1]]
        , geom = "histogram"
        , main = "Blogs"
        , xlab = "Words per Line"
        , ylab = "Count"
        , binwidth = 5) + theme_light()

  qplot(wpl[[2]]
        , geom = "histogram"
        , main = "News"
        , xlab = "Words per Line"
        , ylab = "Count"
        , binwidth = 5) + theme_light()

  qplot(wpl[[3]]
        , geom = "histogram"
        , main = "Twitter"
        , xlab = "Words per Line"
        , ylab = "Count"
        , binwidth = 1) + theme_light()

```

## testing h2

```{r echo=FALSE, message=FALSE, warning=FALSE}

set.seed(8)

# sample size of 0.1% for demonstrative purposes of this report
sampleSize <- 0.001

# sample all three data sets
sampleBlogs <- sample(blogs, length(blogs) * sampleSize, replace = FALSE)
sampleNews <- sample(news, length(news) * sampleSize, replace = FALSE)
sampleTwitter <- sample(twitter, length(twitter) * sampleSize, replace = FALSE)

# remove all non-English characters from the sampled data
sampleBlogs <- iconv(sampleBlogs, "latin1", "ASCII", sub = "")
sampleNews <- iconv(sampleNews, "latin1", "ASCII", sub = "")
sampleTwitter <- iconv(sampleTwitter, "latin1", "ASCII", sub = "")

# combine all three data sets into a single data set and save
sampleData <- c(sampleBlogs, sampleNews, sampleTwitter)
sampleDataFileName <- "data/en_US/en_US.sample.txt"
con <- file(sampleDataFileName, open = "w")
writeLines(sampleData, con)
close(con)

# get number of lines and words from the sample data set
sampleDataLines <- length(sampleData);
sampleDataWords <- sum(stri_count_words(sampleData))

# remove variables no longer needed to free up memory
rm(blogs, news, twitter, sampleBlogs, sampleNews, sampleTwitter)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}

buildCorpus <- function (dataSet) {
    docs <- VCorpus(VectorSource(dataSet))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
    
    # remove URL, Twitter handles and email patterns
    docs <- tm_map(docs, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
    docs <- tm_map(docs, toSpace, "@[^\\s]+")
    docs <- tm_map(docs, toSpace, "\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b")
    
    #
    docs <- tm_map(docs, tolower)
    docs <- tm_map(docs, removeWords, stopwords("english"))
    docs <- tm_map(docs, removePunctuation)
    docs <- tm_map(docs, removeNumbers)
    docs <- tm_map(docs, stripWhitespace)
    docs <- tm_map(docs, PlainTextDocument)
    
    return(docs)
}

# build the corpus and write to disk (RDS)
corpus <- buildCorpus(sampleData)
saveRDS(corpus, file = "data/en_US/en_US.corpus.rds")

# convert corpus to a dataframe and write lines/words to disk (text)
corpusText <- data.frame(text = unlist(sapply(corpus, '[', "content")), stringsAsFactors = FALSE)
con <- file("data/en_US/en_US.corpus.txt", open = "w")
writeLines(corpusText$text, con)
close(con)

rm(sampleData)

```


```{r echo=FALSE, message=FALSE, warning=FALSE}
tdm <- TermDocumentMatrix(corpus)
freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
wordFreq <- data.frame(word = names(freq), freq = freq)

# plot the top 10 most frequent words
g <- ggplot (wordFreq[1:10,], aes(x = reorder(wordFreq[1:10,]$word, -wordFreq[1:10,]$fre),
                                  y = wordFreq[1:10,]$fre ))
g <- g + geom_bar( stat = "Identity" , fill = I("grey50"))
g <- g + geom_text(aes(label = wordFreq[1:10,]$fre), vjust = -0.20, size = 3)
g <- g + xlab("")
g <- g + ylab("Word Frequencies")
g <- g + theme(plot.title = element_text(size = 14, hjust = 0.5, vjust = 0.5),
               axis.text.x = element_text(hjust = 0.5, vjust = 0.5, angle = 45),
               axis.text.y = element_text(hjust = 0.5, vjust = 0.5))
g <- g + ggtitle("10 Most Frequent Words")
print(g)

rm(tdm, freq, wordFreq, g)

```









